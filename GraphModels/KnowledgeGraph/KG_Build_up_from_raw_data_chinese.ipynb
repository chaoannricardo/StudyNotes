{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages needed\n",
    "from codecs import open as codecs_open\n",
    "from opencc import OpenCC\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import monpa\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data from raw json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare raw data for knowledge graph\n",
    "\n",
    "dir_path = \"D:/Windows_Storage/r08546036/36/College/Research/Datasets/wiki_zh/\"\n",
    "dir_num_limit = 5 # limit of data using\n",
    "count = 0\n",
    "file_num = 0\n",
    "\n",
    "id_list = []\n",
    "url_list = []\n",
    "title_list = []\n",
    "text_list = []\n",
    "\n",
    "\n",
    "for index, file_dir in enumerate(tqdm(os.listdir(dir_path))):\n",
    "    count += 1\n",
    "    sub_dir_path = dir_path + file_dir + \"/\"\n",
    "    for file_index, file in enumerate(os.listdir(sub_dir_path)):\n",
    "        # with open(file_path, \"r\", encoding=\"utf8\") as file_to_be_read:\n",
    "        file_to_be_read = codecs_open(sub_dir_path + file, 'r', encoding='utf8')\n",
    "        lines = file_to_be_read.readlines()\n",
    "        for line in lines:\n",
    "            try:\n",
    "                json_file = json.loads(line)\n",
    "                id_list.append(json_file['id'])\n",
    "                url_list.append(json_file['url'])\n",
    "                title_list.append(json_file['title'])\n",
    "                text_list.append(json_file['text'])\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "    # condition to break\n",
    "#     if count >= dir_num_limit:\n",
    "#         break\n",
    "\n",
    "            \n",
    "# create datframe with parsed data\n",
    "df_text = pd.DataFrame({\n",
    "        'id':id_list,\n",
    "        'url':url_list,\n",
    "        'title':title_list,\n",
    "        'text':text_list\n",
    " })\n",
    "\n",
    "print(\"Datafram length\", len(df_text))\n",
    "\n",
    "# translate data to traditional chinese\n",
    "\n",
    "cc = OpenCC('s2twp')\n",
    "\n",
    "for index, text in enumerate(tqdm(df_text.loc[:, 'text'])):\n",
    "    df_text.loc[index, 'text_translated'] = cc.convert(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.to_csv(\"D:/Windows_Storage/r08546036/College/Research/Datasets/wiki2019.csv\",\n",
    "               encoding=\"utf8\",\n",
    "              index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing data and Construct Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# spacy chinese package information: https://spacy.io/models/zh\n",
    "# https://github.com/explosion/spaCy/issues/4577\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download zh_core_web_sm\n",
    "# !python -m spacy download zh_core_web_md\n",
    "import zh_core_web_md\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df_text = pd.read_csv(\"D:/Windows_Storage/r08546036/College/Research/Datasets/wiki2019.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute data with nlp package\n",
    "sentence = df_text.loc[0, 'text_translated']\n",
    "nlp = zh_core_web_md.load()\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# for tok in doc:\n",
    "#     print(tok.text, \"...\", tok.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "#     chunk 1\n",
    "#     I have defined a few empty variables in this chunk. \n",
    "#     prv_tok_dep and prv_tok_text will hold the dependency tag of the previous word \n",
    "#     in the sentence and that previous word itself, respectively. \n",
    "#     prefix and modifier will hold the text that is associated with the subject or the object.\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"        # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"     # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    #############################################################\n",
    "    \n",
    "    for tok in nlp(sent):\n",
    "#         chunk 2\n",
    "#         Next, we will loop through the tokens in the sentence. \n",
    "#         We will first check if the token is a punctuation mark or not. \n",
    "#         If yes, then we will ignore it and move on to the next token. \n",
    "#         If the token is a part of a compound word (dependency tag = “compound”), \n",
    "#         we will keep it in the prefix variable. \n",
    "#         A compound word is a combination of multiple words linked to form a word \n",
    "#         with a new meaning (example – “Football Stadium”, “animal lover”).\n",
    "#         As and when we come across a subject or an object in the sentence, \n",
    "#         we will add this prefix to it. \n",
    "#         We will do the same thing with the modifier words, such as “nice shirt”, “big house”, etc.\n",
    "\n",
    "        # if token is a punctuation mark then move on to the next token\n",
    "        if tok.dep_ != \"punct\":\n",
    "            # check: token is a compound word or not\n",
    "            if tok.dep_ == \"compound\":\n",
    "                prefix = tok.text\n",
    "                # if the previous word was also a 'compound' then add the current word to it\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "            \n",
    "            # check: token is a modifier or not\n",
    "            if tok.dep_.endswith(\"mod\") == True:\n",
    "                modifier = tok.text\n",
    "                # if the previous word was also a 'compound' then add the current word to it\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "            \n",
    "#             chunk 3\n",
    "#             Here, if the token is the subject, \n",
    "#             then it will be captured as the first entity in the ent1 variable. \n",
    "#             Variables such as prefix, modifier, prv_tok_dep, and prv_tok_text will be reset.\n",
    "            if tok.dep_.find(\"subj\") == True:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"            \n",
    "\n",
    "#             chunk4\n",
    "#             Here, if the token is the object, then it will be captured as the second entity in the ent2 variable. \n",
    "#             Variables such as prefix, modifier, prv_tok_dep, and prv_tok_text will again be reset\n",
    "            if tok.dep_.find(\"obj\") == True:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "                \n",
    "#             chunk 5\n",
    "#             Once we have captured the subject and the object in the sentence, \n",
    "#             we will update the previous token and its dependency tag.  \n",
    "            # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "    #############################################################\n",
    "\n",
    "    return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 999/1043191 [07:24<128:46:44,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# loop through data\n",
    "count_limit = 0\n",
    "entity_pairs = []\n",
    "\n",
    "for i in tqdm(df_text.loc[:, 'text_translated']):\n",
    "    entity_pairs.append(get_entities(i))\n",
    "    \n",
    "    # counter\n",
    "    count_limit += 1\n",
    "    if count_limit >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get relation\n",
    "\n",
    "def get_relation(sent):\n",
    "\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # Matcher class object \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    #define the pattern \n",
    "    pattern = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "    matcher.add(\"matching_1\", None, pattern) \n",
    "\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) - 1\n",
    "\n",
    "    span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "    return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 390/1001 [03:47<03:25,  2.97it/s]"
     ]
    }
   ],
   "source": [
    "relations = [get_relation(i) for i in tqdm(df_text.loc[0:count_limit, 'text_translated'])]\n",
    "\n",
    "print(pd.Series(relations).value_counts()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge graph build up\n",
    "\n",
    "# extract subject\n",
    "source = [i[0] for i in entity_pairs]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in entity_pairs]\n",
    "\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directed-graph from a dataframe\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
