results
barplot(Bagging_Model$importance)
Bagging.Prediction$confusion
results <- table(Prediction=Bagging.Prediction$class, Actual=data[select,]$class)
results
Correct_Rate <- sum(diag(results)) / sum(results)
Correct_Rate
# Make Predictions for Test Data
Bagging.Prediction <- predict(Bagging_Model, newdata=data[-select,])
Bagging.Prediction$confusion
results <- table(Prediction=Bagging.Prediction$class, Actual=data[-select,]$class)
results
Correct_Rate <- sum(diag(results)) / sum(results)
Correct_Rate
results
names(results) <- c("Prediction", "Negative", "Positive")
results
# Read Diabetes Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/diabetes.csv")[,-1]
set.seed(102)
select <- sample(1:nrow(data),nrow(data)*0.8)
# Read Diabetes Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/diabetes.csv")[,-1]
set.seed(102)
select <- sample(1:nrow(data),nrow(data)*0.8)
# Build Boosting Model
library(adabag)
set.seed(101)
Boosting_Model <- boosting(class ~ ., data=data[select,])
# Make Predictions for Training Data
Boosting.Prediction <- predict(Boosting_Model, newdata=data[select,])
results <- data.frame(Boosting.Prediction$class, Boosting.Prediction$prob)
names(results) <- c("Prediction", "Negative", "Positive")
results
barplot(Boosting_Model$importance)
Boosting_Model.Error <- errorevol(Boosting_Model, data[select,])                   #计算全体的误差演变
plot(Boosting_Model.Error$error,type="l",
main="AdaBoost Error vs Number of Trees") #对误差演变进行画图
Boosting.Prediction$confusion
results <- table(Prediction=Boosting.Prediction$class, Actual=data[select,]$class)
results
Correct_Rate <- sum(diag(results)) / sum(results)
Correct_Rate
Boosting.Prediction$confusion
# Make Predictions for Test Data
Boosting.Prediction <- predict(Boosting_Model, newdata=data[-select,])
Boosting.Prediction$confusion
results <- table(Prediction=Boosting.Prediction$class, Actual=data[-select,]$class)
results
Correct_Rate <- sum(diag(results)) / sum(results)
Correct_Rate
# Read Diabetes Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/diabetes.csv")[,-1]
set.seed(102)
select <- sample(1:nrow(data),nrow(data)*0.8)
# Build Random Forest Model
library(randomForest)
set.seed(101)
RF_Model <- randomForest(class ~ ., data=data[select,], ntree=300)
install.packages("randomForest")
# Read Diabetes Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/diabetes.csv")[,-1]
set.seed(102)
select <- sample(1:nrow(data),nrow(data)*0.8)
# Build Random Forest Model
library(randomForest)
set.seed(101)
RF_Model <- randomForest(class ~ ., data=data[select,], ntree=300)
# Make Predictions for Training Data
RF.Prediction <- predict(RF_Model, data[select,])
accuracy.rf <- sum(RF.Prediction==data[select,]$class)/length(RF.Prediction)
accuracy.rf
table(RF.Prediction, data[select,]$class)
# Make Predictions for Test Data
RF.Prediction <- predict(RF_Model, data[-select,])
accuracy.rf <- sum(RF.Prediction==data[-select,]$class)/length(RF.Prediction)
accuracy.rf
table(RF.Prediction, data[-select,]$class)
# Read Diabetes Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/diabetes.csv")[,-1]
set.seed(102)
select <- sample(1:nrow(data),nrow(data)*0.8)
# Build Random Forest Model
library(randomForest)
set.seed(101)
RF_Model <- randomForest(class ~ ., data=data[select,], ntree=1500)
# Make Predictions for Training Data
RF.Prediction <- predict(RF_Model, data[select,])
accuracy.rf <- sum(RF.Prediction==data[select,]$class)/length(RF.Prediction)
accuracy.rf
table(RF.Prediction, data[select,]$class)
# Make Predictions for Test Data
RF.Prediction <- predict(RF_Model, data[-select,])
accuracy.rf <- sum(RF.Prediction==data[-select,]$class)/length(RF.Prediction)
accuracy.rf
table(RF.Prediction, data[-select,]$class)
# Read Diabetes Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/diabetes.csv")[,-1]
set.seed(102)
select <- sample(1:nrow(data),nrow(data)*0.8)
# Build Random Forest Model
library(randomForest)
set.seed(101)
RF_Model <- randomForest(class ~ ., data=data[select,], ntree=1500)
# Make Predictions for Training Data
RF.Prediction <- predict(RF_Model, data[select,])
accuracy.rf <- sum(RF.Prediction==data[select,]$class)/length(RF.Prediction)
accuracy.rf
table(RF.Prediction, data[select,]$class)
# Make Predictions for Test Data
RF.Prediction <- predict(RF_Model, data[-select,])
accuracy.rf <- sum(RF.Prediction==data[-select,]$class)/length(RF.Prediction)
accuracy.rf
table(RF.Prediction, data[-select,]$class)
# Read Diabetes Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/diabetes.csv")[,-1]
set.seed(102)
select <- sample(1:nrow(data),nrow(data)*0.8)
# Build Random Forest Model
library(randomForest)
set.seed(101)
RF_Model <- randomForest(class ~ ., data=data[select,], ntree=5000)
# Make Predictions for Training Data
RF.Prediction <- predict(RF_Model, data[select,])
accuracy.rf <- sum(RF.Prediction==data[select,]$class)/length(RF.Prediction)
accuracy.rf
table(RF.Prediction, data[select,]$class)
# Make Predictions for Test Data
RF.Prediction <- predict(RF_Model, data[-select,])
accuracy.rf <- sum(RF.Prediction==data[-select,]$class)/length(RF.Prediction)
accuracy.rf
table(RF.Prediction, data[-select,]$class)
RF_Model.Error <- errorevol(RF_Model, data[select,])                   #计算全体的误差演变
plot(RF_Model.Error$error,type="l",
main="RF Error vs Number of Trees")
# Read Diabetes Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/diabetes.csv")[,-1]
set.seed(102)
select <- sample(1:nrow(data),nrow(data)*0.8)
# Build Random Forest Model
library(randomForest)
set.seed(101)
RF_Model <- randomForest(class ~ ., data=data[select,], ntree=5000)
# Make Predictions for Training Data
RF.Prediction <- predict(RF_Model, data[select,])
accuracy.rf <- sum(RF.Prediction==data[select,]$class)/length(RF.Prediction)
accuracy.rf
table(RF.Prediction, data[select,]$class)
# Make Predictions for Test Data
RF.Prediction <- predict(RF_Model, data[-select,])
accuracy.rf <- sum(RF.Prediction==data[-select,]$class)/length(RF.Prediction)
accuracy.rf
table(RF.Prediction, data[-select,]$class)
# Read Broadband Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/Broadband.csv")[,-1]
# Data Preprocessing
data.y <- data$BROADBAND
data.y
# Read Broadband Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/Broadband.csv")[,-1]
# Data Preprocessing
data.y <- data$BROADBAND
data.n <- data[,-c(5,ncol(data))]
data.c <- data[,c("CHANNEL")]
data.c <- as.factor(data.c)
# Data Preprocessing
data.y <- data$BROADBAND
data.n <- data[,-c(4,ncol(data))]
data.c <- data[,c("CHANNEL")]
data.c <- as.factor(data.c)
# Read Broadband Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/Broadband.csv")[,-1]
# Data Preprocessing
data.y <- data$BROADBAND
data.n <- data[,-c(4,ncol(data))]
data.c <- data[,c("CHANNEL")]
data.c <- as.factor(data.c)
data.c
library("nnet")
dummy.c = as.data.frame(class.ind(data.c))
names(dummy.c) <- c("Channel-1", "Channel-2", "Channel-3", "Channel-4")
#只接受數值性資料
# Read Broadband Dataset
data <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/Broadband.csv")[,-1]
# Data Preprocessing
data.y <- data$BROADBAND
data.n <- data[,-c(4,ncol(data))]
data.c <- data[,c("CHANNEL")]
data.c <- as.factor(data.c)
library("nnet")
dummy.c = as.data.frame(class.ind(data.c))
names(dummy.c) <- c("Channel-1", "Channel-2", "Channel-3", "Channel-4")
# Generate Training & Test Datasets
set.seed(102)
select <- sample(1:nrow(data),nrow(data)*0.8)
train_set.x <- cbind(data.n[select,], dummy.c[select,])
train_set.y <- data.y[select]
test_set.x <- cbind(data.n[-select,], dummy.c[-select,])
test_set.y <- data.y[-select]
install.packages("xgboost")
library(xgboost)
dtrain <- xgb.DMatrix(data = as.matrix(train_set.x), label =  as.matrix(train_set.y))
dtest <- xgb.DMatrix(data = as.matrix(test_set.x))
# Build xgboosting Model  #不接受DataFrame，需做轉換
library(xgboost)
dtrain <- xgb.DMatrix(data = as.matrix(train_set.x), label =  as.matrix(train_set.y))
dtest <- xgb.DMatrix(data = as.matrix(test_set.x))
xgb.params = list(
#col的抽样比例，越高表示每棵树使用的col越多，会增加每棵小树的复杂度
colsample_bytree = 0.5,
# row的抽样比例，越高表示每棵树使用的col越多，会增加每棵小树的复杂度
subsample = 0.5,
booster = "gbtree",
# 树的最大深度，越高表示模型可以长得越深，模型复杂度越高
max_depth = 2,
# boosting会增加被分错的数据权重，而此参数是让权重不会增加的那么快，因此越大会让模型愈保守
eta = 0.03,
# 或用'mae'也可以
eval_metric = "rmse",
objective = "reg:linear",
# 越大，模型会越保守，相对的模型复杂度比较低
gamma = 0)
cv.model = xgb.cv(
params = xgb.params,
data = dtrain,
nfold = 5,     # 5-fold cv
nrounds=200,   # 测试1-100，各个树总数下的模型
# 如果当nrounds < 30 时，就已经有overfitting情况发生，那表示不用继续tune下去了，可以提早停止
early_stopping_rounds = 30,
print_every_n = 20 # 每20个单位才显示一次结果，
)
cv.model = xgb.cv(
params = xgb.params,
data = dtrain,
nfold = 5,     # 5-fold cv (5折交叉驗證)
nrounds=200,   # 测试1-100，各个树总数下的模型
# 如果当nrounds < 30 时，就已经有overfitting情况发生，那表示不用继续tune下去了，可以提早停止
early_stopping_rounds = 30,
print_every_n = 20 # 每20个单位才显示一次结果，
)
tmp <- cv.model$evaluation_log
plot(x=1:nrow(tmp), y= tmp$train_rmse_mean, col='red', xlab="nround", ylab="rmse", main="Avg.Performance in CV")
points(x=1:nrow(tmp), y= tmp$test_rmse_mean, col='blue')
legend("topright", pch=1, col = c("red", "blue"),
legend = c("Train", "Validation") )
best.nrounds = cv.model$best_iteration
best.nrounds
xgb.Model <- xgb.train(paras = xgb.params, data = dtrain, nrounds = best.nrounds)
# Make Predictions for Training Data
xgb.Prediction <- predict(xgb.Model, dtrain)
xgb.Prediction <- ifelse(xgb.Prediction > 0.01, 1, 0)
accuracy.xgb <- sum(xgb.Prediction==train_set.y)/length(xgb.Prediction)
accuracy.xgb
table(xgb.Prediction, train_set.y)
# Make Predictions for Test Data
xgb.Prediction <- predict(xgb.Model, dtest)
xgb.Prediction <- ifelse(xgb.Prediction > 0.01, 1, 0)
accuracy.xgb <- sum(xgb.Prediction==test_set.y)/length(xgb.Prediction)
accuracy.xgb
table(xgb.Prediction, test_set.y)
xgb.Model <- xgb.train(paras = xgb.params, data = dtrain, nrounds = best.nrounds)
xgb.Model <- xgb.train(paras = xgb.params, data = dtrain, nrounds = best.nrounds)
# Make Predictions for Training Data
xgb.Prediction <- predict(xgb.Model, dtrain)
xgb.Prediction <- ifelse(xgb.Prediction > 0.01, 1, 0)
accuracy.xgb <- sum(xgb.Prediction==train_set.y)/length(xgb.Prediction)
accuracy.xgb
table(xgb.Prediction, train_set.y)
# Make Predictions for Test Data
xgb.Prediction <- predict(xgb.Model, dtest)
xgb.Prediction <- ifelse(xgb.Prediction > 0.01, 1, 0)
accuracy.xgb <- sum(xgb.Prediction==test_set.y)/length(xgb.Prediction)
accuracy.xgb
table(xgb.Prediction, test_set.y)
# Extract All Numerical Attributes
# iris2 <- iris[,sapply(iris,is.numeric)]
iris2 <- iris[,-5]
iris2
iris2
set.seed(1234)
kmeans.result <- kmeans(iris2, 3)
kmeans.result
kmeans.result
table(iris$Species, kmeans.result$cluster)
plot(iris2, col=kmeans.result$cluster)
#Find 10 Largest Distances between Objects and Cluster Centers
kmeans.result$centers
centers <- kmeans.result$centers[kmeans.result$cluster,]
head(centers)
distances <- sqrt(rowSums((iris2 - centers)^2))
outliers <- order(distances, decreasing=T)[1:10]
outliers
#Find 10 Largest Distances between Objects and Cluster Centers
kmeans.result$centers
centers <- kmeans.result$centers[kmeans.result$cluster,]
head(centers)
distances <- sqrt(rowSums((iris2 - centers)^2))
outliers <- order(distances, decreasing=T)[1:10]
outliers
outliers <- order(distances, decreasing=T)[1:10]
outliers
iris2[outliers,]
plot(iris2[c("Sepal.Length", "Petal.Width")], col=kmeans.result$cluster)
points(kmeans.result$centers[,c("Sepal.Length", "Petal.Width")], col=1:3, pch=8, cex=2)
points(iris2[outliers,c("Sepal.Length", "Petal.Width")], col=4, pch='+', cex=2)
iris2 <- iris[,-5]
#Sample 40 Records
index <- sample(1:nrow(iris2),40)
irissample <- iris2[index,]
install.packages("fpc")
install.packages("NbClust")
#dist函數 distance matrix compute
hclust.result <- hclust(dist(irissample), method= "ward.D2")
plot(hclust.result, labels=iris$Species[index])
rect.hclust(hclust.result, k=3, border="red")
groups <- cutree(hclust.result, k=3)
table(iris$Species[index], groups)
groups <- cutree(hclust.result, k=3)
table(iris$Species[index], groups)
# 階層式===================================================
#Sample 40 Records
set.seed(100)
index <- sample(1:nrow(iris2),40)
irissample <- iris2[index,]
#dist函數 distance matrix compute
hclust.result <- hclust(dist(irissample), method= "ward.D2")
plot(hclust.result, labels=iris$Species[index])
rect.hclust(hclust.result, k=3, border="red")
groups <- cutree(hclust.result, k=3)
table(iris$Species[index], groups)
library(fpc)
pamk.result <- pamk(iris2)
pamk.result$nc
library(NbClust)
NbClust.result <- NbClust(data=iris2, distance="euclidean",
min.nc=2, max.nc=10,
method="ward.D2", index="all")
age_income <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/Age_Income.csv", header=T, sep=",")
age_income_ID <- age_income[,1]
age_income_s <- age_income[,-c(1,4)]
hclust.result <- hclust(dist(age_income_s), method= "ward.D2")
hclust.result
plot(hclust.result, labels=age_income_ID)
rect.hclust(hclust.result, k=3, border="red")
groups <- cutree(hclust.result, k=3)
plot(age_income_s, col=groups)
library(NbClust)
NbClust.result <- NbClust(data=age_income_s, distance="euclidean",
min.nc=2, max.nc=10,
method="ward.D2", index="all")
# 決定群集數的方法I
library(fpc)
pamk.result <- pamk(age_income_s)
pamk.result$nc
# K-Means Clustering
age_income <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/Age_Income.csv",header=T,sep=",")
age_income_s <- age_income[,-c(1,4)]
# K-Means Clustering
age_income <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/Age_Income.csv",header=T,sep=",")
age_income_s <- age_income[,-c(1,4)]
kmd <- kmeans(age_income_s, centers=3)
kmd
kmd$cluster
kmd$centers
# Draw Graph
plot(age_income_s, col=kmd$cluster)
points(kmd$centers, col=1:3, pch=8, cex=2)
# Clustering Evaluation
# R2
kmd$totss
kmd$withinss
kmd$tot.withinss
#R2 <- (kmd$totss - kmd$tot.withinss) / kmd$totss
R2 <- kmd$betweenss / kmd$totss
R2
# Test R2 for Different Different K
result <- list()
for (i in 2:5){
kmd <- kmeans(age_income_s, centers=i)
R2 <- kmd$betweenss / kmd$totss
result[[paste('k=',i,sep='')]] = R2
}
result
# Clustering Evaluation
# R2
kmd$totss
kmd$withinss
kmd$tot.withinss
#R2 <- (kmd$totss - kmd$tot.withinss) / kmd$totss
R2 <- kmd$betweenss / kmd$totss
R2
result <- list()
for (i in 2:5){
kmd <- kmeans(age_income_s, centers=i)
R2 <- kmd$betweenss / kmd$totss
result[[paste('k=',i,sep='')]] = R2
}
result
# Test Silhouette Coefficient for Different Different K
library(cluster)
kmd <- kmeans(age_income_s, centers=3)
sil <- silhouette(kmd$cluster, dist(age_income_s))
sil
mean(sil[,'sil_width'])
result <- list()
for (i in 2:5){
kmd <- kmeans(age_income_s, centers=i)
sil <- silhouette(kmd$cluster, dist(age_income_s))
result[[paste('k=',i,sep='')]] <- mean(sil[,'sil_width'])
}
result
# Test Silhouette Coefficient for Different Different K
library(cluster)
kmd <- kmeans(age_income_s, centers=3)
sil <- silhouette(kmd$cluster, dist(age_income_s))
sil
mean(sil[,'sil_width'])
result <- list()
for (i in 2:5){
kmd <- kmeans(age_income_s, centers=i)
sil <- silhouette(kmd$cluster, dist(age_income_s))
result[[paste('k=',i,sep='')]] <- mean(sil[,'sil_width'])
}
result
# Autumatically Finding K Using PAM
library(fpc)
pamk.result <- pamk(age_income_s)
pamk.result$pamobject$clustering
pamk.result$nc
# Graph for PAM
layout(matrix(c(1,2),1,2))
plot(pamk.result$pamobject)
layout(matrix(1))
library(NbClust)
NbClust.result <- NbClust(data=iris2, distance="euclidean",
min.nc=2, max.nc=10,
method="ward.D2", index="all")
age_income <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/Age_Income4.csv",header=T,sep=",")
plot(age_income)
set.seed(105)
kmd <- kmeans(age_income, centers=2)
kmd$centers
plot(age_income, col=kmd$cluster)
points(kmd$centers, col=1:3, pch=8, cex=2)
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
age_income_sn <- as.data.frame(lapply(age_income, normalize))
plot(age_income_sn)
set.seed(105)
kmd <- kmeans(age_income_sn, centers=2)
kmd$centers
plot(age_income_sn, col=kmd$cluster)
points(kmd$centers, col=1:3, pch=8, cex=2)
age_income <- read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/Age_Income4.csv",header=T,sep=",")
plot(age_income)
set.seed(105)
kmd <- kmeans(age_income, centers=2)
kmd$centers
plot(age_income, col=kmd$cluster)
points(kmd$centers, col=1:3, pch=8, cex=2)
# Normalization
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
age_income_sn <- as.data.frame(lapply(age_income, normalize))
plot(age_income_sn)
set.seed(105)
kmd <- kmeans(age_income_sn, centers=2)
kmd$centers
plot(age_income_sn, col=kmd$cluster)
points(kmd$centers, col=1:3, pch=8, cex=2)
install.packages("arules")
library(arules)
tr <- read.transactions("C:/Users/richi/Desktop/Temp/R_Lesson/bank.csv", format = "single", cols = c(1,2),sep=",")
inspect(tr)
library(arules)
tr <- read.transactions("C:/Users/richi/Desktop/Temp/R_Lesson/bank.csv", format = "single", cols = c(1,2),sep=",")
inspect(tr)
rules <- apriori(tr,parameter=list(supp=0.1,conf=0.1,minlen=2,maxlen=10))
inspect(rules)
rulesData=as(rules,"data.frame")
rulesData
rule2=rules[which(rules@quality$lift>1.1)]
inspect(rule2)
rule3=sort(rules,by="lift",decreasing=T)
inspect(rule3)
rule2=rules[which(rules@quality$lift>1.1)]
inspect(rule2)
rule3=sort(rules,by="lift",decreasing=T)
inspect(rule3)
library(arulesViz)
plot(rules, shading="lift")
install.packages("arulesViz")
library(arulesViz)
plot(rules, shading="lift")
install.packages("arulesSequences")
library(arulesSequences)
data=read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/bankS.csv")
library(arulesSequences)
data=read.csv("C:/Users/richi/Desktop/Temp/R_Lesson/bankS.csv")
data1=data.frame(item=factor(data$ITEM))
tran=as(data1,"transactions")
transactionInfo(tran)$sequenceID=data$SID
transactionInfo(tran)$eventID=data$EID
inspect(tran)
rule.seq = cspade(tran, parameter = list(support = 0.2, maxlen = 10))
inspect(rule.seq)
rules=rule.seq[which(rule.seq@quality$support>0.5)]
inspect(rules)
rule.seq=sort(rule.seq,by="support",decreasing=T)
inspect(rule.seq)
