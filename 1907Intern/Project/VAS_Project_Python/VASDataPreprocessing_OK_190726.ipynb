{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import chardet\n",
    "import numpy as np\n",
    "import math\n",
    "# coding=utf-8\n",
    "\n",
    "#查詢目錄位置\n",
    "print( os.getcwd()  )\n",
    "\n",
    "Folder_Path = \"Z:/VPA-13C OAD Data/1_OK\"   #要拼接的文件夾及其完整路徑，注意不要包含中文\n",
    "SaveFile_Path = 'C:/Users/1907075/Project/VAS_Project_Python'    #拼接後要保存的文件路徑\n",
    "SaveFile_Name = 'all_0720_OK.csv'                                        #合並後要保存的文件名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判斷編碼型態\n",
    "\n",
    "def find_encoding(fname):\n",
    "    r_file = open(fname, 'rb').read() # 'b', meaning 'binary'.\n",
    "    result = chardet.detect(r_file) # chardet.detect() 函式就可以偵測字串「最有可能」的編碼\n",
    "    charenc = result['encoding'] #識別編碼\n",
    "    return charenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料清理，增加資料\n",
    "def data_Cleaning(dfname,filename):\n",
    "    #filename = file_list[0]  #取檔名\n",
    "\n",
    "    dfname.insert(1,'filename' ,filename ) #插入檔名\n",
    "    #dfname[np.isnan(dfname['Vacuum Data_VG001_Exponent'])]=0\n",
    "    \n",
    "    # Vacuum Align System (真空貼合系統)\n",
    "    # 換算壓力值公式 : log10(Mantissa*10^Exponent)，總共7組 gauge pressure sensor ( Mantissa(尾數) ; Exponent(指數))\n",
    "    dfname.insert(2,'VG001' ,np.log10(dfname['Vacuum Data_VG001_Mantissa']*(np.power(10.,(dfname['Vacuum Data_VG001_Exponent']) )  )  ))   #插入VG001\n",
    "    dfname.insert(3,'VG002' ,np.log10(dfname['Vacuum Data_VG002_Mantissa']*(np.power(10.,(dfname['Vacuum Data_VG002_Exponent']) )  )  ))   #插入VG002\n",
    "    dfname.insert(4,'VG003' ,np.log10(dfname['Vacuum Data_VG003_Mantissa']*(np.power(10.,(dfname['Vacuum Data_VG003_Exponent']) )  )  ))   #插入VG003    \n",
    "    dfname.insert(5,'VG111' ,np.log10(dfname['Vacuum Data_VG111_Mantissa']*(np.power(10.,(dfname['Vacuum Data_VG111_Exponent']) )  )  ))   #插入VG111        \n",
    "    dfname.insert(6,'VG921' ,np.log10(dfname['Vacuum Data_VG921_Mantissa']*(np.power(10.,(dfname['Vacuum Data_VG921_Exponent']) )  )  ))   #插入VG921        \n",
    "    dfname.insert(7,'VG922' ,np.log10(dfname['Vacuum Data_VG922_Mantissa']*(np.power(10.,(dfname['Vacuum Data_VG923_Exponent']) )  )  ))   #插入VG922        \n",
    "    dfname.insert(8,'VG923' ,np.log10(dfname['Vacuum Data_VG923_Mantissa']*(np.power(10.,(dfname['Vacuum Data_VG922_Exponent']) )  )  ))   #插入VG923        \n",
    "    \n",
    "    #取代上午/下午\n",
    "      # df.loc[df.First_name != 'Bill', 'name_match'] = 'Mis-Match'  \n",
    "    dfname.loc[dfname.DateTime.str.contains(\"上午\") , 'DateTime24'] =  dfname.DateTime.str.replace('上午','') +\"AM\" \n",
    "    dfname.loc[dfname.DateTime.str.contains(\"下午\") , 'DateTime24'] =  dfname.DateTime.str.replace('下午','') +\"PM\"        \n",
    "\n",
    "    #轉換 DateTime 24小時制\n",
    "    dfname.DateTime24 = pd.to_datetime( dfname.DateTime24 ).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "     # 移動排序 DateTime24\n",
    "    cols = list(dfname)\n",
    "    cols.insert(0,cols.pop(cols.index('DateTime24')))\n",
    "    dfname = dfname.loc[:,cols]\n",
    "\n",
    "   # dfname['new_DateTime']=dfname.new_DateTime(1)\n",
    "\n",
    "    # Row 排序\n",
    "    #dfname.sort(['DateTime'], ascending=[1])\n",
    "    dfname.sort_values(by=['filename', 'DateTime24'] )  \n",
    "    \n",
    "    #  刪除重複row    \n",
    "    dfname.drop_duplicates( ['filename', 'DateTime24' ] , keep = 'first' ,inplace=True)   \n",
    "    \n",
    "    # 插入rownum\n",
    "    dfname.insert(1,'rownum' , np.arange(len(dfname)) ) \n",
    "    \n",
    "    return dfname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改當前工作目錄\n",
    "os.chdir(Folder_Path)\n",
    "#將該文件夾下的所有文件名存入一個列表\n",
    "file_list = os.listdir()\n",
    " \n",
    "#讀取第一個CSV文件並包含表頭\n",
    "my_encoding = find_encoding(Folder_Path +'/'+ file_list[0])\n",
    "df = pd.read_csv(Folder_Path +'/'+ file_list[0], encoding=my_encoding)   #編碼默認UTF-8，若亂碼自行更改\n",
    "\n",
    "#將讀取的第一個CSV文件寫入合並後的文件保存\n",
    "dfClean=data_Cleaning(df,file_list[0])\n",
    "dfClean.to_csv(SaveFile_Path+'/'+ SaveFile_Name,encoding=\"utf_8\",index=False)  # 儲存 Column Name\n",
    "    \n",
    "#循環遍歷列表中各個CSV文件名，並追加到合並後的文件\n",
    "for i,j in enumerate(file_list):\n",
    "    # 讀檔\n",
    "    df = pd.read_csv(Folder_Path + '/'+ file_list[i], encoding=my_encoding)  \n",
    "    \n",
    "    # 清洗資料\n",
    "    dfClean=data_Cleaning(df, j)\n",
    "        \n",
    "   \n",
    "    # Save All\n",
    "    dfClean.to_csv(SaveFile_Path+'/'+ SaveFile_Name,encoding=\"utf_8\",index=False, header=False, mode='a+')  # header=False (不儲存Column Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dfClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data using pandas build in read csv function\n",
    "#df_city_time_series = pd.read_csv('../input/City_time_series.csv',parse_dates=['Date'])\n",
    "df_city_time_series = dfClean \n",
    "# drop null values in ZHVIPerSqft_AllHomes because we are interested in this column\n",
    "df_city_time_series = df_city_time_series.dropna(subset=['VG001'])\n",
    "# print the head of our data set\n",
    "df_city_time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series data source: fpp pacakge in R.\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"C:/Users/1907075/Project/VAS_Project_Python/all_0720_OK.csv\", parse_dates=['rownum'], index_col='rownum')\n",
    "#df=dfClean\n",
    "# Draw Plot\n",
    "def plot_df(df, x, y, title=\"\", xlabel='rownum', ylabel='VG001', dpi=1500):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    plt.plot(x, y, color='tab:red')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n",
    "\n",
    "plot_df(df, x=df.index, y=df.VG001, title='VAS Serial.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(20,4), dpi=100)\n",
    "#pd.read_csv(r'C:\\Users\\bwich\\Google_Cloud\\AIA_Project-VAS\\Data\\all_0720.csv', parse_dates=['rownum'], index_col='rownum').plot(title='Trend Only', legend=False, ax=axes[0])\n",
    "\n",
    "pd.read_csv(\"C:/Users/1907075/Project/VAS_Project_Python/all_0720_OK.csv\", parse_dates=['rownum'], index_col='rownum').plot(title='Seasonality Only', legend=False, ax=axes[1])\n",
    "\n",
    "#pd.read_csv(r'C:\\Users\\bwich\\Google_Cloud\\AIA_Project-VAS\\Data\\all_0720.csv', parse_dates=['rownum'], index_col='rownum').plot(title='Trend and Seasonality', legend=False, ax=axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "cc =  pd.read_csv(\"C:/Users/1907075/Project/VAS_Project_Python/all_0720_OK.csv\", parse_dates=['rownum'], index_col='rownum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data check. \n",
    "\n",
    "print(cc.head())\n",
    "print(cc.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I observed an conflict in the name 'class'. Therefore, I have changed the name from class to category\n",
    "\n",
    "#cc= cc.rename(columns={'Class': 'Category'})\n",
    "cc[\"Category\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_obs = cc.loc[cc.Category==0]    #Data frame with normal observation\n",
    "ano_obs = cc.loc[cc.Category==1]    #Data frame with anomalous observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The given dataframe 'cc' is divided into three sets\n",
    "\n",
    "# Training set: train_features\n",
    "\n",
    "# Test observations/features: X_test\n",
    "\n",
    "# Test labels: Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一旦類SVM訓練只有一個類的觀察。 在這種情況下，通過首次200,000次正常交易觀察來訓練算法。\n",
    "# 剩下的觀察與異常觀察合併以創建測試集。\n",
    "# Once class SVM is trained with the observations of only one class. In this case, the algorithm is trained with first 200,000 observation of normal transactions. The remaining observations are merged with the anomalous observation to create a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once class SVM is trained with the observations of only one class. In this case, the algorithm is trained with \n",
    "# first 200,000 observation of normal transactions. The remaining observation is merged with the anomalous observation \n",
    "# to create a test set. \n",
    "\n",
    "train_feature = nor_obs.loc[0:200000, :]\n",
    "train_feature = train_feature.drop('Category', 1)\n",
    "Y_1 = nor_obs.loc[200000:, 'Category']\n",
    "Y_2 = ano_obs['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
