{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reference: \n",
    "    https://finthon.com/learn-cnn-two-tfrecord-read-data/\n",
    "    https://finthon.com/learn-cnn-three-resnet-prediction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 匯入圖片資料並輸出成tfrecord檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "設置路徑\n",
    "# 將需分類之圖片目錄放置Working Directory於之下，Folder以Int作為命名\n",
    "'''\n",
    "# 图片路径，两组标签都在该目录下\n",
    "cwd = r\"./OM/\"\n",
    "# tfrecord文件保存路径\n",
    "file_path = r\"./\"\n",
    "# 每个tfrecord存放图片个数\n",
    "bestnum = 10000\n",
    "# 第几个图片\n",
    "num = 0\n",
    "# 第几个TFRecord文件\n",
    "recordfilenum = 0\n",
    "# 将labels放入到classes中\n",
    "classes = []\n",
    "for i in os.listdir(cwd):\n",
    "    classes.append(i)\n",
    "# tfrecords格式文件名\n",
    "ftrecordfilename = (\"traindata_63.tfrecords-%.3d\" % recordfilenum)\n",
    "writer = tf.python_io.TFRecordWriter(os.path.join(file_path, ftrecordfilename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tfrecord 製造函數\n",
    "'''\n",
    "for index, name in enumerate(classes):\n",
    "    class_path = os.path.join(cwd, name)\n",
    "    for img_name in os.listdir(class_path):\n",
    "        num = num + 1\n",
    "        if num > bestnum:   \n",
    "            num = 1\n",
    "            recordfilenum += 1\n",
    "            ftrecordfilename = (\"traindata_63.tfrecords-%.3d\" % recordfilenum)\n",
    "            writer = tf.python_io.TFRecordWriter(os.path.join(file_path, ftrecordfilename))\n",
    "        \n",
    "        img_path = os.path.join(class_path, img_name)  # 每一个图片的地址\n",
    "        img = Image.open(img_path, 'r')\n",
    "        img_raw = img.tobytes()  # 将图片转化为二进制格式\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(feature={\n",
    "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n",
    "                'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n",
    "            }))\n",
    "        writer.write(example.SerializeToString())  # 序列化为字符串\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自tfrecord輸入資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tfrecord 輸入函數\n",
    "'''\n",
    "import tensorflow as tf\n",
    " \n",
    "def read_and_decode_tfrecord(filename):\n",
    "    filename_deque = tf.train.string_input_producer(filename)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_deque)\n",
    "    features = tf.parse_single_example(serialized_example, features={\n",
    "        'label': tf.FixedLenFeature([], tf.int64),\n",
    "        'img_raw': tf.FixedLenFeature([], tf.string)})\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    img = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "    img = tf.reshape(img, [640, 480, 3])\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim.nets as nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义模型保存地址，batch_sizes设置的小一点训练效果更好，将当前目录下的tfrecord文件放入列表中:\n",
    "# tf.train.shuffle_batch: 随机打乱队列里面的数据顺序\n",
    "# num_threads: 表示线程数\n",
    "# capacity\" 表示队列的容量，在这里设置成10000\n",
    "# min_after_dequeue: 队列里保留的最小数据量，并且控制着随机的程度，\n",
    "设置成9900的意思是，当队列中的数据出列100个，剩下9900个的时候，就要重新补充100个数据进来并打乱顺序。\n",
    "如果你要按顺序导入队列，改成tf.train.batch函数，并删除min_after_dequeue参数。\n",
    "这些参数都要根据自己的电脑配置进行相应的设置。\n",
    "接下来将label值进行onehot编码，直接调用tf.one_hot函数。因为我们这里有2类，depth设置成2:\n",
    "'''\n",
    "\n",
    "save_dir = r\"./train_image_63.model\"  # 模型保存路径\n",
    "batch_size_ = 2\n",
    "lr = tf.Variable(0.0001, dtype=tf.float32)  # 学习速率\n",
    "x = tf.placeholder(tf.float32, [None, 640, 480, 3])  # 图片大小为224*224*3\n",
    "y_ = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "'''\n",
    "train_list = ['traindata_63.tfrecords-000', 'traindata_63.tfrecords-001', 'traindata_63.tfrecords-002',\n",
    "              'traindata_63.tfrecords-003', 'traindata_63.tfrecords-004', 'traindata_63.tfrecords-005',\n",
    "              'traindata_63.tfrecords-006', 'traindata_63.tfrecords-007', 'traindata_63.tfrecords-008',\n",
    "              'traindata_63.tfrecords-009', 'traindata_63.tfrecords-010', 'traindata_63.tfrecords-011',\n",
    "              'traindata_63.tfrecords-012', 'traindata_63.tfrecords-013', 'traindata_63.tfrecords-014',\n",
    "              'traindata_63.tfrecords-015', 'traindata_63.tfrecords-016', 'traindata_63.tfrecords-017',\n",
    "              'traindata_63.tfrecords-018', 'traindata_63.tfrecords-019', 'traindata_63.tfrecords-020',\n",
    "              'traindata_63.tfrecords-021']    #制作成的所有tfrecord数据，每个最多包含1000个图片数据\n",
    "'''\n",
    "\n",
    "train_list = ['traindata_63.tfrecords-000']\n",
    "\n",
    "# 随机打乱顺序\n",
    "img, label = read_and_decode_tfrecord(train_list)\n",
    "img_batch, label_batch = tf.train.shuffle_batch([img, label], num_threads=2, batch_size=batch_size_, capacity=10000,\n",
    "                                                min_after_dequeue=9900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "接下来将label值进行onehot编码，直接调用tf.one_hot函数。因为我们这里有100类，depth设置成100:\n",
    "'''\n",
    "# 将label值进行onehot编码\n",
    "one_hot_labels = tf.one_hot(indices=tf.cast(y_, tf.int32), depth=2)\n",
    "pred, end_points = nets.resnet_v2.resnet_v2_50(x, num_classes=2, is_training=True)\n",
    "pred = tf.reshape(pred, shape=[-1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# nets.resnet_v2.resnet_v2_50: 直接调用ResNet_50网络\n",
    "# num_classes等于类别总数\n",
    "# is_training表示我们是否要训练网络里面固定层的参数，True表示所有参数都重新训练，False表示只训练后面几层的参数。\n",
    "网络搭好后，我们继续定义损失函数和优化器，损失函数选择sigmoid交叉熵，优化器选择Adam：\n",
    "'''\n",
    "# 定义损失函数和优化器\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=one_hot_labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确度\n",
    "a = tf.argmax(pred, 1)\n",
    "b = tf.argmax(one_hot_labels, 1)\n",
    "correct_pred = tf.equal(a, b)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 创建一个协调器，管理线程\n",
    "    coord = tf.train.Coordinator()\n",
    "    # 启动QueueRunner,此时文件名队列已经进队\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        b_image, b_label = sess.run([img_batch, label_batch])\n",
    "        _, loss_, y_t, y_p, a_, b_ = sess.run([optimizer, loss, one_hot_labels, pred, a, b], feed_dict={x: b_image,\n",
    "                                                                                                        y_: b_label})\n",
    "        print('step: {}, train_loss: {}'.format(i, loss_))\n",
    "        if i % 20 == 0:\n",
    "            _loss, acc_train = sess.run([loss, accuracy], feed_dict={x: b_image, y_: b_label})\n",
    "            print('--------------------------------------------------------')\n",
    "            print('step: {}  train_acc: {}  loss: {}'.format(i, acc_train, _loss))\n",
    "            print('--------------------------------------------------------')\n",
    "            if i == 200:\n",
    "                saver.save(sess, save_dir, global_step=i)\n",
    "            #elif i == 300000:\n",
    "            #    saver.save(sess, save_dir, global_step=i)\n",
    "            #elif i == 400000:\n",
    "            #    saver.save(sess, save_dir, global_step=i)\n",
    "                break\n",
    "    coord.request_stop()\n",
    "    # 其他所有线程关闭之后，这一函数才能返回\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用模型進行預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim.nets as nets\n",
    "from PIL import Image\n",
    "import os\n",
    " \n",
    "test_dir = r'./test'     # 原始的test文件夹，含带预测的图片\n",
    "model_dir = r'./train_image_63.model-300000'     # 模型地址\n",
    "test_txt_dir = r'./test.txt'     # 原始的test.txt文件\n",
    "result_dir = r'./result.txt'     # 生成输出结果\n",
    "x = tf.placeholder(tf.float32, [None, 640, 480, 3])\n",
    "\n",
    "'''\n",
    "classes = ['1', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24',\n",
    "           '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40',\n",
    "           '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57',\n",
    "           '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73',\n",
    "           '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9',\n",
    "           '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']      # 标签顺序\n",
    "'''\n",
    "\n",
    "classes = ['0', '1']      # 标签顺序\n",
    "\n",
    "pred, end_points = nets.resnet_v2.resnet_v2_50(x, num_classes=2, is_training=True)\n",
    "pred = tf.reshape(pred, shape=[-1, 2])\n",
    "a = tf.argmax(pred, 1)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, model_dir)\n",
    "    with open(test_txt_dir, 'r') as f:\n",
    "        data = f.readlines()\n",
    "        for i in data:\n",
    "            test_name = i.split()[0]\n",
    "            for pic in os.listdir(test_dir):\n",
    "                if pic == test_name:\n",
    "                    img_path = os.path.join(test_dir, pic)\n",
    "                    img = Image.open(img_path)\n",
    "                    img = img.resize((640, 480))\n",
    "                    img = tf.reshape(img, [1, 640, 480, 3])\n",
    "                    img1 = tf.reshape(img, [1, 640, 480, 3])\n",
    "                    img = tf.cast(img, tf.float32) / 255.0\n",
    "                    b_image, b_image_raw = sess.run([img, img1])\n",
    "                    t_label = sess.run(a, feed_dict={x: b_image})\n",
    "                    index_ = t_label[0]\n",
    "                    predict = classes[index_]\n",
    "                    with open(result_dir, 'a') as f1:\n",
    "                        print(test_name, predict, file=f1)\n",
    "                    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
